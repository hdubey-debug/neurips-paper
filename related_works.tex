\section{Related Works}

Traditional n-gram-based metrics such as BLEU \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge}, and METEOR \cite{banerjee2005meteor} evaluate text generation through lexical overlap and local word order. BLEU measures n-gram precision with brevity penalties, capturing local chronology but limited by rigid lexical matching. ROUGE variants emphasize recall; ROUGE-N evaluates n-gram overlap, while ROUGE-L utilizes the Longest Common Subsequence (LCS) at sentence-level, yet remains sensitive to lexical variation and sentence-length discrepancies. METEOR integrates lexical alignment via synonyms and stems, computing precision-recall harmonics with fragmentation penalties for local word order. CIDEr \cite{vedantam2015cider} addresses lexical variability by consensus-based TF-IDF weighting across multiple references but proves impractical for dense, long descriptions due to labor-intensive annotation. SPICE \cite{anderson2016spice} evaluates semantic propositions using graph overlaps, effectively handling paraphrasing but neglecting fluency, grammar, and narrative chronology critical for video descriptions.

Embedding-based metrics compare texts in semantic vector spaces, leveraging pretrained models to capture semantic similarity beyond lexical matches. Early methods like BERTScore \cite{zhang2019bertscore}, MoverScore \cite{zhao2019moverscore}, and SBERT \cite{reimers2019sentence} recognize paraphrases but are constrained by limited context windows, complicating their direct application to extended narratives. Recent decoder-based models (e.g., nv-embed-v2 \cite{lee2024nv}, Linq-Embed-Mistral \cite{choi2024linq}, SFR-Embedding-Mistral \cite{meng2024sfrembedding}, Jasper and Stella \cite{zhang2024jasper}) offer significantly larger context windows and robust global embeddings, excelling at paragraph-level semantic assessments. However, reliance on global embeddings and cosine similarity overlooks local content alignment, detailed information accuracy, and chronological coherence, allowing subtle inaccuracies or misordered events to remain undetected.

Multimodal embedding metrics like EMScore \cite{shi2022emscore} and PAC-S \cite{sarto2023positive} employ vision-language models (e.g., CLIP \cite{radford2021learning}) to evaluate semantic alignment between visuals and generated captions. EMScore combines coarse and fine-grained multimodal matches for accurate short-caption evaluation, whereas PAC-S, fine-tuned via contrastive learning, closely aligns with human judgments. Despite their effectiveness in short-form tasks, these metrics face computational challenges and methodological limitations when scaling to dense, extended narratives, struggling with complex chronology and segment-level coherence without significant adaptations.

Recent evaluation approaches increasingly leverage Large Language Models (LLMs), categorized into component-based and holistic judge methods. Component-based methods (e.g., AutoDQ \cite{wang2024tarsier}, VAD-Score \cite{dubey2025leveraging}) use LLMs for semantic extraction and entailment checks, effectively addressing semantic variation and many-to-one mapping challenges; however, they do not evaluate chronology of events. Nonetheless, their effectiveness depends heavily on extraction accuracy, consistency across model updates, scalability with dense content, and reliance on comprehensive references. Conversely, holistic methods (e.g., ChatGPT-based scoring \cite{achiam2023gpt}) provide overall quality assessments directly from LLMs, theoretically addressing complex evaluation dimensions comprehensively. However, they suffer from ambiguity in score calibration, sensitivity to prompting nuances, consistency issues across model versions, limited interpretability, and practical constraints including reproducibility and cost.

