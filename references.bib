@inproceedings{he2024ma,
  title={Ma-lmm: Memory-augmented large multimodal model for long-term video understanding},
  author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13504--13514},
  year={2024}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220101},
  year={2024},
  publisher={Springer}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{cheng2024videollama,
  title={Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@inproceedings{ataallah2024goldfish,
  title={Goldfish: Vision-language understanding of arbitrarily long videos},
  author={Ataallah, Kirolos and Shen, Xiaoqian and Abdelrahman, Eslam and Sleiman, Essam and Zhuge, Mingchen and Ding, Jian and Zhu, Deyao and Schmidhuber, J{\"u}rgen and Elhoseiny, Mohamed},
  booktitle={European Conference on Computer Vision},
  pages={251--267},
  year={2024},
  organization={Springer}
}

@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@inproceedings{liu2024st,
  title={St-llm: Large language models are effective temporal learners},
  author={Liu, Ruyang and Li, Chen and Tang, Haoran and Ge, Yixiao and Shan, Ying and Li, Ge},
  booktitle={European Conference on Computer Vision},
  pages={1--18},
  year={2024},
  organization={Springer}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@article{lin2023video,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{ye2024mplug,
  title={mplug-owl3: Towards long image-sequence understanding in multi-modal large language models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}

@article{yao2024minicpm,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{zhou2024mlvu,
  title={Mlvu: A comprehensive benchmark for multi-task long video understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}

@inproceedings{tapaswi2016movieqa,
  title={Movieqa: Understanding stories in movies through question-answering},
  author={Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4631--4640},
  year={2016}
}

@article{wang2024lvbench,
  title={Lvbench: An extreme long video understanding benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}

@article{fu2024video,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@article{nagrani2024neptune,
  title={Neptune: The Long Orbit to Benchmarking Long Video Understanding},
  author={Nagrani, Arsha and Zhang, Mingda and Mehran, Ramin and Hornung, Rachel and Gundavarapu, Nitesh Bharadwaj and Jha, Nilpa and Myers, Austin and Zhou, Xingyi and Gong, Boqing and Schmid, Cordelia and others},
  journal={arXiv preprint arXiv:2412.09582},
  year={2024}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22195--22206},
  year={2024}
}

@article{rawal2024cinepile,
  title={Cinepile: A long video question answering dataset and benchmark},
  author={Rawal, Ruchit and Saifullah, Khalid and Farr{\'e}, Miquel and Basri, Ronen and Jacobs, David and Somepalli, Gowthami and Goldstein, Tom},
  journal={arXiv preprint arXiv:2405.08813},
  year={2024}
}

@article{ataallah2024infinibench,
  title={Infinibench: A comprehensive benchmark for large multimodal models in very long video understanding},
  author={Ataallah, Kirolos and Gou, Chenhui and Abdelrahman, Eslam and Pahwa, Khushbu and Ding, Jian and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2406.19875},
  year={2024}
}

@article{wu2024longvideobench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={28828--28857},
  year={2024}
}

@inproceedings{tan2025allvb,
  title={ALLVB: All-in-One Long Video Understanding Benchmark},
  author={Tan, Xichen and Luo, Yuanjing and Ye, Yunfan and Liu, Fang and Cai, Zhiping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={7},
  pages={7211--7219},
  year={2025}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@inproceedings{shi2022emscore,
  title={Emscore: Evaluating video captioning via coarse-grained and fine-grained embedding matching},
  author={Shi, Yaya and Yang, Xu and Xu, Haiyang and Yuan, Chunfeng and Li, Bing and Hu, Weiming and Zha, Zheng-Jun},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={17929--17938},
  year={2022}
}

@article{chen2023dartscore,
  title={DARTScore: DuAl-Reconstruction Transformer for Video Captioning Evaluation},
  author={Chen, Yuxin and Zhang, Ziqi and Qi, Zhongang and Yuan, Chunfeng and Wang, Jie and Shan, Ying and Li, Bing and Hu, Weiming and Qie, Xiaohu and Wu, Jianping},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={34},
  number={4},
  pages={2041--2055},
  year={2023},
  publisher={IEEE}
}

@inproceedings{sarto2023positive,
  title={Positive-augmented contrastive learning for image and video captioning evaluation},
  author={Sarto, Sara and Barraco, Manuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6914--6924},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@article{zhang2024jasper,
  title={Jasper and Stella: distillation of SOTA embedding models},
  author={Zhang, Dun and Li, Jiacheng and Zeng, Ziyang and Wang, Fulong},
  journal={arXiv preprint arXiv:2412.19048},
  year={2024}
}

@inproceedings{yi2020improving,
  title={Improving image captioning evaluation by considering inter references variance},
  author={Yi, Yanzhi and Deng, Hangyu and Hu, Jinglu},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={985--994},
  year={2020}
}

@article{zhao2019moverscore,
  title={MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance},
  author={Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M and Eger, Steffen},
  journal={arXiv preprint arXiv:1909.02622},
  year={2019}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{choi2024linq,
  title={Linq-Embed-Mistral Technical Report},
  author={Choi, Chanyeol and Kim, Junseong and Lee, Seolhwa and Kwon, Jihoon and Gu, Sangmo and Kim, Yejin and Cho, Minkyung and Sohn, Jy-yong},
  journal={arXiv preprint arXiv:2412.03223},
  year={2024}
}

@article{meng2024sfrembedding,
  title={Sfrembedding-mistral: enhance text retrieval with transfer learning},
  author={Meng, Rui and Liu, Ye and Joty, Shafiq Rayhan and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih},
  journal={Salesforce AI Research Blog},
  volume={3},
  pages={6},
  year={2024}
}

@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{wang2024tarsier,
  title={Tarsier: Recipes for training and evaluating large video description models},
  author={Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao},
  journal={arXiv preprint arXiv:2407.00634},
  year={2024}
}

@inproceedings{dubey2025leveraging,
  title={Leveraging Textual Memory and Key Frame Reasoning for Full Video Understanding Using Off-the-Shelf LLMs and VLMs (Student Abstract)},
  author={Dubey, Harsh and Pack, Chulwoo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={28},
  pages={29351--29352},
  year={2025}
}

@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023}
}

@article{frohmann2024segment,
  title={Segment Any Text: A universal approach for robust, efficient and adaptable sentence segmentation},
  author={Frohmann, Markus and Sterner, Igor and Vuli{\'c}, Ivan and Minixhofer, Benjamin and Schedl, Markus},
  journal={arXiv preprint arXiv:2406.16678},
  year={2024}
}

@article{lee2024nv,
  title={Nv-embed: Improved techniques for training llms as generalist embedding models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@inproceedings{rohrbach2015dataset,
  title={A dataset for movie description},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3202--3212},
  year={2015}
}



